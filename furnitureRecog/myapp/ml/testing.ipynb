{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-06T20:22:44.437705Z",
     "start_time": "2025-07-06T20:22:40.087611Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3k/wcc4c2m97_d363_yy26qzp7c0000gn/T/ipykernel_41927/3054360764.py:194: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/Users/oxydear/Documents/Ivan's Mac/IDEs/PyCharm/furnitureRecog/venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/90 : < :, Epoch 0.11/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:\n",
      "–¢–µ–∫—Å—Ç: '–ö—É–ø–∏—Ç–µ –Ω–æ–≤—ã–π iPhone 15 Pro Max –∏ Samsung S24'\n",
      "–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã: ['iPhone 15 Pro Max', 'Samsung S24']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertConfig, BertForTokenClassification\n",
    "from transformers import PreTrainedTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "import torch\n",
    "\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "train_data = [\n",
    "    {\"text\": \"–°–∫–∏–¥–∫–∞ 20% –Ω–∞ –≤—Å–µ –º–æ–¥–µ–ª–∏ Redmi Note 12 Pro –¥–æ –∫–æ–Ω—Ü–∞ –Ω–µ–¥–µ–ª–∏\", \"tags\": [0,0,0,0,0,1,1,1,1,0,0,0]},\n",
    "    {\"text\": \"–í –ø—Ä–æ–¥–∞–∂—É –ø–æ—Å—Ç—É–ø–∏–ª –Ω–æ–≤—ã–π ASUS ROG Strix G17 —Å –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–æ–º AMD\", \"tags\": [0,0,0,0,1,1,1,1,0,0,0]},\n",
    "    {\"text\": \"–ö–∞–º–µ—Ä–∞ Alpha A7 IV –æ—Ç Sony –¥–µ–ª–∞–µ—Ç –ø–æ—Ç—Ä—è—Å–∞—é—â–∏–µ —Å–Ω–∏–º–∫–∏ –ø—Ä–∏ —Å–ª–∞–±–æ–º –æ—Å–≤–µ—â–µ–Ω–∏–∏\", \"tags\": [0,1,1,1,0,0,0,0,0,0,0,0]},\n",
    "    {\"text\": \"Band 8 –æ—Ç Huawei –¥–µ—Ä–∂–∏—Ç –∑–∞—Ä—è–¥ –¥–æ 2 –Ω–µ–¥–µ–ª—å –≤ —Ä–µ–∂–∏–º–µ –æ–∂–∏–¥–∞–Ω–∏—è\", \"tags\": [1,1,0,0,0,0,0,0,0,0,0,0]},\n",
    "    {\"text\": \"–ì–¥–µ –Ω–∞–π—Ç–∏ –∑–∞–º–µ–Ω—É –∞–∫–∫—É–º—É–ª—è—Ç–æ—Ä–∞ –¥–ª—è MX Master 3S?\", \"tags\": [0,0,0,0,0,1,1,1]},\n",
    "    {\"text\": \"–ö—Ç–æ-–Ω–∏–±—É–¥—å —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–ª Mi Electric Scooter 4 Pro –≤ –º–æ—Ä–æ–∑?\", \"tags\": [0,0,1,1,1,1,1,0,0]},\n",
    "    {\"text\": \"Apple –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª–∞ Watch Ultra 2 —Å –Ω–æ–≤—ã–º –¥–∞—Ç—á–∏–∫–æ–º\", \"tags\": [0,0,1,1,1,0,0,0]},\n",
    "    {\"text\": \"Samsung Galaxy Z Flip5 –ø–æ–ª—É—á–∏–ª –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–º–µ—Ä—ã\", \"tags\": [1,1,1,1,0,0,0]},\n",
    "    {\"text\": \"–¢–æ–ª—å–∫–æ —á—Ç–æ —Ä–∞—Å–ø–∞–∫–æ–≤–∞–ª EOS R6 Mark II - –∫–∞–º–µ—Ä–∞ –ø—Ä–æ—Å—Ç–æ –∫–æ—Å–º–æ—Å!\", \"tags\": [0,0,0,1,1,1,1,0,0,0,0]},\n",
    "    {\"text\": \"–ì–¥–µ –∫—É–ø–∏—Ç—å PlayStation 5 —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π –≤ –ú–æ—Å–∫–≤–µ?\", \"tags\": [0,0,1,1,0,0,0,0]},\n",
    "    {\"text\": \"–ü—Ä–æ–±–ª–µ–º—ã —Å –¥—Ä–∞–π–≤–µ—Ä–∞–º–∏ –¥–ª—è LaserJet Pro MFP M283fdw –Ω–∞ Windows 11\", \"tags\": [0,0,0,0,1,1,1,1,0,0,0]},\n",
    "    {\"text\": \"GeForce RTX 4090 –ø–µ—Ä–µ–≥—Ä–µ–≤–∞–µ—Ç—Å—è –≤ –∏–≥—Ä–∞—Ö\", \"tags\": [1,1,1,0,0,0]},\n",
    "    {\"text\": \"–î–ª—è –º–æ–Ω—Ç–∞–∂–∞ –≤–∏–¥–µ–æ –±–µ—Ä–∏—Ç–µ Core i9-14900K –∏ ROG Strix Z790-E\", \"tags\": [0,0,0,0,1,1,0,1,1,1]},\n",
    "    {\"text\": \"SSD 990 Pro 2TB - –ª—É—á—à–∏–π –≤—ã–±–æ—Ä –¥–ª—è –∏–≥—Ä\", \"tags\": [1,1,1,1,0,0,0,0,0]},\n",
    "    {\"text\": \"UltraSharp U2723QE –æ—Ç Dell - –∏–¥–µ–∞–ª–µ–Ω –¥–ª—è —Ü–≤–µ—Ç–æ–∫–æ—Ä—Ä–µ–∫—Ü–∏–∏\", \"tags\": [1,1,0,0,0,0,0,0]},\n",
    "    {\"text\": \"Series 9 –æ—Ç Braun –±—Ä–µ–µ—Ç –ª—É—á—à–µ –º–æ–µ–π —Å—Ç–∞—Ä–æ–π –º–æ–¥–µ–ª–∏\", \"tags\": [1,1,0,0,0,0,0,0,0]}\n",
    "]\n",
    "\n",
    "train_data += [\n",
    "    {\"text\": \"–ö—É–ø–∏—Ç–µ –Ω–æ–≤—ã–π iPhone 15 Pro Max –≤ –º–∞–≥–∞–∑–∏–Ω–µ\", \"tags\": [0, 0, 1, 1, 1, 1, 0, 0]},\n",
    "    {\"text\": \"–ó–∞–∫–∞–∂–∏—Ç–µ –Ω–æ—É—Ç–±—É–∫ Lenovo IdeaPad 5 —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π\", \"tags\": [0, 0, 1, 1, 1, 0, 0]},\n",
    "    {\"text\": \"–í –ø—Ä–æ–¥–∞–∂–µ –ø–æ—è–≤–∏–ª–∏—Å—å Samsung Galaxy S24 –∏ Xiaomi 14\", \"tags\": [0, 0, 0, 1, 1, 1, 0, 1, 1]},\n",
    "    {\"text\": \"–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º —Ç–µ–ª–µ–≤–∏–∑–æ—Ä Sony Bravia XR-55A80J\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–ü–æ–ø—Ä–æ–±—É–π—Ç–µ –Ω–∞—É—à–Ω–∏–∫–∏ AirPods Pro 2\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–ù–æ–≤—ã–π –ø—ã–ª–µ—Å–æ—Å Dyson V15 Absolute\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–ò–≥—Ä–∞ The Last of Us Part II —Ç–µ–ø–µ—Ä—å –≤ –ø—Ä–æ–¥–∞–∂–µ\", \"tags\": [0, 1, 1, 1, 1, 1, 1, 0, 0, 0]},\n",
    "    {\"text\": \"–§–∏—Ç–Ω–µ—Å-–±—Ä–∞—Å–ª–µ—Ç Huawei Band 8\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–§–æ—Ç–æ–∞–ø–ø–∞—Ä–∞—Ç Canon EOS R6 Mark II\", \"tags\": [0, 1, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ú–∏–∫—Ä–æ–≤–æ–ª–Ω–æ–≤–∫–∞ LG MS-2042DB\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–°–º–∞—Ä—Ç-—á–∞—Å—ã Apple Watch Series 9\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–≠–ª–µ–∫—Ç—Ä–∏—á–µ—Å–∫–∏–π —á–∞–π–Ω–∏–∫ Bosch TWK7201\", \"tags\": [0, 0, 1, 1]},\n",
    "    {\"text\": \"–ò–≥—Ä–æ–≤–∞—è –∫–æ–Ω—Å–æ–ª—å PlayStation 5 Slim\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–ú–æ–Ω–∏—Ç–æ—Ä Dell UltraSharp U2723QE\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ö–æ—Ñ–µ–≤–∞—Ä–∫–∞ DeLonghi Dinamica Plus\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ú—ã—à—å Logitech MX Master 3S\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ö–ª–∞–≤–∏–∞—Ç—É—Ä–∞ Keychron Q3 Pro\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ü–ª–∞–Ω—à–µ—Ç iPad Air 2024\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω–∞—è –∫–Ω–∏–≥–∞ PocketBook 740\", \"tags\": [0, 0, 1, 1]},\n",
    "    {\"text\": \"–ü—Ä–∏–Ω—Ç–µ—Ä HP LaserJet Pro MFP M283fdw\", \"tags\": [0, 1, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–•–æ–ª–æ–¥–∏–ª—å–Ω–∏–∫ Bosch Serie 6 VitaFresh\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–°—Ç–∏—Ä–∞–ª—å–Ω–∞—è –º–∞—à–∏–Ω–∞ Samsung WW90TA046AE\", \"tags\": [0, 0, 1, 1]},\n",
    "    {\"text\": \"–ü–æ—Å—É–¥–æ–º–æ–π–∫–∞ Electrolux ESF9453LOX\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–†–æ–±–æ—Ç-–ø—ã–ª–µ—Å–æ—Å Roborock S8 Pro Ultra\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–£–º–Ω–∞—è –∫–æ–ª–æ–Ω–∫–∞ –Ø–Ω–¥–µ–∫—Å –°—Ç–∞–Ω—Ü–∏—è 2\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–≠—Ö–æ–ª–æ—Ç Garmin Striker Plus 7SV\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–î—Ä–æ–Ω DJI Mavic 3 Pro\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–≠–ª–µ–∫—Ç—Ä–æ—Å–∞–º–æ–∫–∞—Ç Xiaomi Mi Electric Scooter 4 Pro\", \"tags\": [0, 1, 1, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ì–∏—Ä–æ—Å–∫—É—Ç–µ—Ä Smart Balance Wheel 10\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–≠–ª–µ–∫—Ç—Ä–æ–±—Ä–∏—Ç–≤–∞ Braun Series 9\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–§–µ–Ω Dyson Supersonic HD15\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ó—É–±–Ω–∞—è —â–µ—Ç–∫–∞ Oral-B iO Series 9\", \"tags\": [0, 0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–í–µ—Å—ã Withings Body+\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–¢–µ—Ä–º–æ–º–µ—Ç—Ä B.Well WF-4000\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–ì–µ–π–º–ø–∞–¥ Xbox Wireless Controller\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–†–æ—É—Ç–µ—Ä ASUS RT-AX86U Pro\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"SSD-–Ω–∞–∫–æ–ø–∏—Ç–µ–ª—å Samsung 990 Pro\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–í–∏–¥–µ–æ–∫–∞—Ä—Ç–∞ NVIDIA GeForce RTX 4090\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ü—Ä–æ—Ü–µ—Å—Å–æ—Ä Intel Core i9-14900K\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ú–∞—Ç–µ—Ä–∏–Ω—Å–∫–∞—è –ø–ª–∞—Ç–∞ ASUS ROG Strix Z790-E\", \"tags\": [0, 0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ë–ª–æ–∫ –ø–∏—Ç–∞–Ω–∏—è Corsair RM1000x\", \"tags\": [0, 0, 1, 1]},\n",
    "    {\"text\": \"–ö–æ—Ä–ø—É—Å NZXT H7 Elite\", \"tags\": [0, 1, 1, 1]},\n",
    "    {\"text\": \"–ö—É–ª–µ—Ä –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä–∞ Noctua NH-D15\", \"tags\": [0, 0, 0, 1, 1]},\n",
    "    {\"text\": \"–û–ø–µ—Ä–∞—Ç–∏–≤–Ω–∞—è –ø–∞–º—è—Ç—å Kingston Fury Beast DDR5\", \"tags\": [0, 0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–í–Ω–µ—à–Ω–∏–π –∂–µ—Å—Ç–∫–∏–π –¥–∏—Å–∫ WD My Passport 5TB\", \"tags\": [0, 0, 0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–§–ª–µ—à–∫–∞ SanDisk Ultra Fit 256GB\", \"tags\": [0, 1, 1, 1, 1]},\n",
    "    {\"text\": \"–ö–∞—Ä—Ç–∞ –ø–∞–º—è—Ç–∏ Sony TOUGH 128GB\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–ö–∞–±–µ–ª—å USB-C Anker PowerLine III\", \"tags\": [0, 0, 1, 1, 1]},\n",
    "    {\"text\": \"–î–æ–∫-—Å—Ç–∞–Ω—Ü–∏—è CalDigit TS4\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–®—É—Ä—É–ø–æ–≤–µ—Ä—Ç Makita DDF487SHE\", \"tags\": [0, 1, 1]},\n",
    "    {\"text\": \"–ü–µ—Ä—Ñ–æ—Ä–∞—Ç–æ—Ä Bosch GBH 2-28\", \"tags\": [0, 1, 1, 1]}\n",
    "]\n",
    "\n",
    "# 3. –°–æ–∑–¥–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ (–≤–∞—à –∫–æ–¥ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)\n",
    "def build_tokenizer(train_data):\n",
    "    vocab = set()\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "    for item in train_data:\n",
    "        vocab.update(item[\"text\"].split())\n",
    "    vocab = {word: idx for idx, word in enumerate(special_tokens + sorted(vocab))}\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    return PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    )\n",
    "\n",
    "tokenizer = build_tokenizer(train_data)\n",
    "\n",
    "def prepare_dataset(data, tokenizer, max_length=128):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        tokens = item[\"text\"].split()\n",
    "        tags = item[\"tags\"]\n",
    "        encoding = tokenizer(\n",
    "            tokens,\n",
    "            is_split_into_words=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        word_ids = encoding.word_ids()\n",
    "        aligned_labels = []\n",
    "        current_word = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                aligned_labels.append(-100)\n",
    "            elif word_id != current_word:\n",
    "                aligned_labels.append(tags[word_id])\n",
    "                current_word = word_id\n",
    "            else:\n",
    "                aligned_labels.append(tags[word_id] if tags[word_id] != -100 else -100)\n",
    "        if len(aligned_labels) < max_length:\n",
    "            aligned_labels += [-100] * (max_length - len(aligned_labels))\n",
    "        input_ids.append(encoding['input_ids'].squeeze())\n",
    "        attention_masks.append(encoding['attention_mask'].squeeze())\n",
    "        labels.append(torch.tensor(aligned_labels))\n",
    "    return {\n",
    "        'input_ids': torch.stack(input_ids),\n",
    "        'attention_mask': torch.stack(attention_masks),\n",
    "        'labels': torch.stack(labels)\n",
    "    }\n",
    "\n",
    "processed_data = prepare_dataset(train_data, tokenizer)\n",
    "dataset = Dataset.from_dict({\n",
    "    'input_ids': processed_data['input_ids'].numpy(),\n",
    "    'attention_mask': processed_data['attention_mask'].numpy(),\n",
    "    'labels': processed_data['labels'].numpy()\n",
    "})\n",
    "\n",
    "config = BertConfig(\n",
    "    vocab_size=len(tokenizer),\n",
    "    hidden_size=128,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=4,\n",
    "    intermediate_size=256,\n",
    "    max_position_embeddings=512,\n",
    "    num_labels=2\n",
    ")\n",
    "model = BertForTokenClassification(config).to(device)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_ner_model\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=3e-4,\n",
    "    save_steps=200,\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\",\n",
    "    disable_tqdm=False,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "def predict_products(text, model, tokenizer, max_length=128):\n",
    "    tokens = text.split()\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length'\n",
    "    ).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "    word_ids = inputs.word_ids()\n",
    "    products = []\n",
    "    current_product = []\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "        if predictions[i] == 1:\n",
    "            current_product.append(tokens[word_id])\n",
    "        elif current_product:\n",
    "            products.append(\" \".join(current_product))\n",
    "            current_product = []\n",
    "    if current_product:\n",
    "        products.append(\" \".join(current_product))\n",
    "    return products\n",
    "\n",
    "test_text = \"–ö—É–ø–∏—Ç–µ –Ω–æ–≤—ã–π iPhone 15 Pro Max –∏ Samsung S24\"\n",
    "print(\"\\nüß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏:\")\n",
    "print(f\"–¢–µ–∫—Å—Ç: '{test_text}'\")\n",
    "print(\"–ò–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã:\", predict_products(test_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
