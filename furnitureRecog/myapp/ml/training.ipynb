{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T06:56:27.406213Z",
     "start_time": "2025-07-07T06:56:17.997297Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n",
      "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é...\n",
      "\n",
      "–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oxydear/Documents/Ivan's Mac/IDEs/PyCharm/furnitureRecog/venv/lib/python3.11/site-packages/transformers/training_args.py:1604: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ü§ó Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/3k/wcc4c2m97_d363_yy26qzp7c0000gn/T/ipykernel_43354/2982115909.py:244: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='2' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 2/75 : < :, Epoch 0.20/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
      "\n",
      "1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ URL —Å–∞–π—Ç–∞\n",
      "2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≤–≤–µ–¥–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\n",
      "3. –í—ã—Ö–æ–¥\n",
      "\n",
      "–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ 'custom_ner_model'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import string\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import BertConfig, BertForTokenClassification\n",
    "from transformers import PreTrainedTokenizerFast, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from datasets import Dataset\n",
    "from tokenizers import Tokenizer, models, pre_tokenizers\n",
    "import torch\n",
    "import numpy as np\n",
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# 1. –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 2. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Ç–µ–∫—Å—Ç–∞ —Å —Å–∞–π—Ç–∞\n",
    "async def async_get_website_text(url):\n",
    "    \"\"\"–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Ñ—É–Ω–∫—Ü–∏–∏ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å —Å–∞–π—Ç–∞\"\"\"\n",
    "    try:\n",
    "        async with async_playwright() as p:\n",
    "            browser = await p.chromium.launch()\n",
    "            page = await browser.new_page()\n",
    "\n",
    "            # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞\n",
    "            await page.set_viewport_size({\"width\": 1280, \"height\": 720})\n",
    "            await page.goto(url, wait_until=\"networkidle\", timeout=90000)\n",
    "\n",
    "            # –û–∂–∏–¥–∞–Ω–∏–µ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏\n",
    "            try:\n",
    "                await page.wait_for_load_state(\"networkidle\", timeout=30000)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            content = await page.content()\n",
    "            await browser.close()\n",
    "\n",
    "            soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "            # –£–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–Ω—É–∂–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
    "            for element in soup(['script', 'style', 'nav', 'footer',\n",
    "                               'iframe', 'svg', 'img', 'button']):\n",
    "                element.decompose()\n",
    "\n",
    "            text = soup.get_text(separator='\\n', strip=True)\n",
    "            return text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ —Å–∞–π—Ç–∞: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_website_text(url):\n",
    "    \"\"\"–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –æ–±–µ—Ä—Ç–∫–∞ –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏\"\"\"\n",
    "    try:\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –∑–∞–ø—É—â–µ–Ω–Ω—ã–π event loop\n",
    "        try:\n",
    "            loop = asyncio.get_event_loop()\n",
    "        except RuntimeError:\n",
    "            loop = None\n",
    "\n",
    "        # –ï—Å–ª–∏ loop –∑–∞–ø—É—â–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º create_task\n",
    "        if loop and loop.is_running():\n",
    "            async def wrapper():\n",
    "                return await async_get_website_text(url)\n",
    "            future = asyncio.ensure_future(wrapper())\n",
    "            return loop.run_until_complete(future)\n",
    "        else:\n",
    "            # –ò–Ω–∞—á–µ —Å–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π loop\n",
    "            return asyncio.run(async_get_website_text(url))\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –≤ —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–π –æ–±–µ—Ä—Ç–∫–µ: {e}\")\n",
    "        return None\n",
    "\n",
    "# 3. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "def load_and_prepare_data(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö\n",
    "        if not isinstance(data, list):\n",
    "            print(\"–û—à–∏–±–∫–∞: –î–∞–Ω–Ω—ã–µ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –º–∞—Å—Å–∏–≤–æ–º –æ–±—ä–µ–∫—Ç–æ–≤\")\n",
    "            return None\n",
    "\n",
    "        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –∑–∞–ø–∏—Å–∏\n",
    "        valid_data = []\n",
    "        for item in data:\n",
    "            if not isinstance(item, dict):\n",
    "                continue\n",
    "            if \"text\" not in item or \"result\" not in item:\n",
    "                continue\n",
    "            if not isinstance(item[\"text\"], str) or not isinstance(item[\"result\"], list):\n",
    "                continue\n",
    "\n",
    "            # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤\n",
    "            valid_results = [res for res in item[\"result\"] if isinstance(res, str) and res.strip()]\n",
    "            if not valid_results:\n",
    "                continue\n",
    "\n",
    "            valid_data.append({\n",
    "                \"text\": item[\"text\"],\n",
    "                \"result\": valid_results\n",
    "            })\n",
    "\n",
    "        if not valid_data:\n",
    "            print(\"–û—à–∏–±–∫–∞: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ –ø—Ä–æ–≤–µ—Ä–∫–∏\")\n",
    "            return None\n",
    "\n",
    "        return valid_data\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_tags_from_result(example):\n",
    "    text = example[\"text\"]\n",
    "    result_products = example[\"result\"]\n",
    "\n",
    "    # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —Å—Ç—Ä–æ–∫–∏ (—Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å \\n)\n",
    "    lines = [line.strip() for line in text.split('\\n') if line.strip()]\n",
    "\n",
    "    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–≥–∏ (0 –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–æ–∫–∏)\n",
    "    tags = [0] * len(lines)\n",
    "\n",
    "    # –û—á–∏—â–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "    cleaned_products = []\n",
    "    for product in result_products:\n",
    "        # –£–¥–∞–ª—è–µ–º –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É\n",
    "        cleaned = product.strip().strip(string.punctuation).lower()\n",
    "        if cleaned:\n",
    "            cleaned_products.append(cleaned)\n",
    "\n",
    "    # –ü–æ–º–µ—á–∞–µ–º —Å—Ç—Ä–æ–∫–∏, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç–æ–≤–∞—Ä—ã\n",
    "    for i, line in enumerate(lines):\n",
    "        # –û—á–∏—â–∞–µ–º —Å—Ç—Ä–æ–∫—É –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è\n",
    "        cleaned_line = line.strip().strip(string.punctuation).lower()\n",
    "\n",
    "        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ —Å—Ç—Ä–æ–∫–∞ –ª—é–±–æ–π –∏–∑ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "        for product in cleaned_products:\n",
    "            if product in cleaned_line:\n",
    "                tags[i] = 1\n",
    "                break  # –ü–æ–º–µ—á–∞–µ–º —Å—Ç—Ä–æ–∫—É –∫–∞–∫ —Ç–æ–≤–∞—Ä –∏ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ —Å–ª–µ–¥—É—é—â–µ–π\n",
    "\n",
    "    return {\n",
    "        \"text\": '\\n'.join(lines),  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç —Å –æ—á–∏—â–µ–Ω–Ω—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏\n",
    "        \"tags\": tags\n",
    "    }\n",
    "\n",
    "# 4. –°–æ–∑–¥–∞–Ω–∏–µ –∏ –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "def train_model(train_data):\n",
    "    if not train_data:\n",
    "        print(\"–û—à–∏–±–∫–∞: –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\")\n",
    "        return None, None\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å\n",
    "    vocab = set()\n",
    "    special_tokens = [\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"\\n\"]\n",
    "\n",
    "    for item in train_data:\n",
    "        for word in item[\"text\"].split():\n",
    "            vocab.add(word)\n",
    "\n",
    "    vocab = {word: idx for idx, word in enumerate(special_tokens + sorted(vocab))}\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    tokenizer = PreTrainedTokenizerFast(\n",
    "        tokenizer_object=tokenizer,\n",
    "        unk_token=\"[UNK]\",\n",
    "        pad_token=\"[PAD]\",\n",
    "        cls_token=\"[CLS]\",\n",
    "        sep_token=\"[SEP]\",\n",
    "        mask_token=\"[MASK]\",\n",
    "    )\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    formatted_data = []\n",
    "    for example in train_data:\n",
    "        tagged = create_tags_from_result(example)\n",
    "        if tagged:\n",
    "            formatted_data.append(tagged)\n",
    "\n",
    "    def prepare_dataset(data, tokenizer, max_length=256):\n",
    "        input_ids = []\n",
    "        attention_masks = []\n",
    "        labels = []\n",
    "\n",
    "        for item in data:\n",
    "            tokens = item[\"text\"].split()\n",
    "            tags = item[\"tags\"]\n",
    "\n",
    "            encoding = tokenizer(\n",
    "                tokens,\n",
    "                is_split_into_words=True,\n",
    "                truncation=True,\n",
    "                max_length=max_length,\n",
    "                padding='max_length',\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "\n",
    "            word_ids = encoding.word_ids()\n",
    "            aligned_labels = []\n",
    "\n",
    "            for i, word_id in enumerate(word_ids):\n",
    "                if word_id is None:\n",
    "                    aligned_labels.append(-100)\n",
    "                else:\n",
    "                    if word_id >= len(tokens):\n",
    "                        aligned_labels.append(-100)\n",
    "                    elif tokens[word_id] == '\\n':\n",
    "                        aligned_labels.append(-100)\n",
    "                    else:\n",
    "                        aligned_labels.append(tags[word_id] if word_id < len(tags) else -100)\n",
    "\n",
    "            aligned_labels += [-100] * (max_length - len(aligned_labels))\n",
    "\n",
    "            input_ids.append(encoding['input_ids'].squeeze())\n",
    "            attention_masks.append(encoding['attention_mask'].squeeze())\n",
    "            labels.append(torch.tensor(aligned_labels))\n",
    "\n",
    "        return {\n",
    "            'input_ids': torch.stack(input_ids),\n",
    "            'attention_mask': torch.stack(attention_masks),\n",
    "            'labels': torch.stack(labels)\n",
    "        }\n",
    "\n",
    "    processed_data = prepare_dataset(formatted_data, tokenizer)\n",
    "    dataset = Dataset.from_dict({\n",
    "        'input_ids': processed_data['input_ids'].numpy(),\n",
    "        'attention_mask': processed_data['attention_mask'].numpy(),\n",
    "        'labels': processed_data['labels'].numpy()\n",
    "    })\n",
    "\n",
    "    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å\n",
    "    config = BertConfig(\n",
    "        vocab_size=len(tokenizer),\n",
    "        hidden_size=128,\n",
    "        num_hidden_layers=4,\n",
    "        num_attention_heads=4,\n",
    "        intermediate_size=256,\n",
    "        max_position_embeddings=512,\n",
    "        num_labels=2\n",
    "    )\n",
    "    model = BertForTokenClassification(config).to(device)\n",
    "\n",
    "    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—É—á–µ–Ω–∏—è\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./custom_ner_model\",\n",
    "        num_train_epochs=15,\n",
    "        per_device_train_batch_size=4,\n",
    "        learning_rate=2e-4,\n",
    "        save_steps=200,\n",
    "        logging_steps=10,\n",
    "        report_to=\"none\",\n",
    "        disable_tqdm=False,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        optim=\"adamw_torch\",\n",
    "        no_cuda=True if device.type == 'mps' else False,\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "\n",
    "    print(\"\\n–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
    "        return model, tokenizer\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 5. –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ–≤–∞—Ä–æ–≤\n",
    "def predict_products(text, model, tokenizer, max_length=256):\n",
    "    if not text or not model or not tokenizer:\n",
    "        print(\"–û—à–∏–±–∫–∞: –ù–µ–≤–µ—Ä–Ω—ã–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è\")\n",
    "        return []\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    tokens = text.split()\n",
    "    inputs = tokenizer(\n",
    "        tokens,\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding='max_length'\n",
    "    ).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=-1)[0].cpu().numpy()\n",
    "\n",
    "    word_ids = inputs.word_ids()\n",
    "    current_product = []\n",
    "    products = []\n",
    "\n",
    "    for i, word_id in enumerate(word_ids):\n",
    "        if word_id is None:\n",
    "            continue\n",
    "\n",
    "        if word_id >= len(tokens):\n",
    "            continue\n",
    "\n",
    "        token = tokens[word_id]\n",
    "\n",
    "        if predictions[i] == 1:\n",
    "            current_product.append(token)\n",
    "        elif current_product:\n",
    "            products.append(\" \".join(current_product))\n",
    "            current_product = []\n",
    "\n",
    "    if current_product:\n",
    "        products.append(\" \".join(current_product))\n",
    "\n",
    "    return products\n",
    "\n",
    "# –û—Å–Ω–æ–≤–Ω–æ–π –ø–æ—Ç–æ–∫ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è\n",
    "if __name__ == \"__main__\":\n",
    "    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    train_data = load_and_prepare_data(\"/Users/oxydear/Documents/Ivan's Mac/IDEs/PyCharm/furnitureRecog/furnitureRecog/myapp/output.json\")\n",
    "    if not train_data:\n",
    "        exit()\n",
    "\n",
    "    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    print(\"–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –∫ –æ–±—É—á–µ–Ω–∏—é...\")\n",
    "    model, tokenizer = train_model(train_data)\n",
    "    if not model or not tokenizer:\n",
    "        exit()\n",
    "\n",
    "    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
    "    while True:\n",
    "        print(\"\\n1. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ URL —Å–∞–π—Ç–∞\")\n",
    "        print(\"2. –ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞—Ç—å –Ω–∞ –≤–≤–µ–¥–µ–Ω–Ω–æ–º —Ç–µ–∫—Å—Ç–µ\")\n",
    "        print(\"3. –í—ã—Ö–æ–¥\")\n",
    "        choice = input(\"–í—ã–±–µ—Ä–∏—Ç–µ –≤–∞—Ä–∏–∞–Ω—Ç: \")\n",
    "\n",
    "        if choice == '1':\n",
    "            website_url = input(\"–í–≤–µ–¥–∏—Ç–µ URL —Å–∞–π—Ç–∞: \")\n",
    "            print(f\"–ê–Ω–∞–ª–∏–∑ —Å–∞–π—Ç–∞ {website_url}...\")\n",
    "            website_text = get_website_text(website_url)\n",
    "\n",
    "            if website_text:\n",
    "                found_products = predict_products(website_text, model, tokenizer)\n",
    "\n",
    "                if found_products:\n",
    "                    print(\"\\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã:\")\n",
    "                    for i, product in enumerate(found_products, 1):\n",
    "                        print(f\"{i}. {product}\")\n",
    "                else:\n",
    "                    print(\"–¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
    "            else:\n",
    "                print(\"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç —Å —Å–∞–π—Ç–∞\")\n",
    "\n",
    "        elif choice == '2':\n",
    "            text = input(\"–í–≤–µ–¥–∏—Ç–µ —Ç–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: \")\n",
    "            found_products = predict_products(text, model, tokenizer)\n",
    "\n",
    "            if found_products:\n",
    "                print(\"\\n–ù–∞–π–¥–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã:\")\n",
    "                for i, product in enumerate(found_products, 1):\n",
    "                    print(f\"{i}. {product}\")\n",
    "            else:\n",
    "                print(\"–¢–æ–≤–∞—Ä—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã\")\n",
    "\n",
    "        elif choice == '3':\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            print(\"–ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞\")\n",
    "\n",
    "    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "    model.save_pretrained(\"./custom_ner_model\")\n",
    "    tokenizer.save_pretrained(\"./custom_ner_model\")\n",
    "    print(\"\\n–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ –ø–∞–ø–∫–µ 'custom_ner_model'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
